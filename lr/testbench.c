/**
* AUTHOR: MAW
* DATE: 06/12/2017 - 07/12/2017
* COMMENTS: This file contains a simple implementation in C of LR classifier.
* Data generated from Python code in order to test.
**/
#include "lr.h"

// #include "train_data.h"
// #include "train_label.h"
// #include "test_data.h"
// #include "test_label.h"
	

double sw_classify(double *x, double *weights){
	double temp = .0;
	unsigned int i, j;
	for (i = 0; i < NB_FEATURES; i++)
			temp += weights[i]*x[i];
	return sigmoid(temp);
}

int main(){
	unsigned i, j;

	// Data generated from python file
	double x_train[][NB_FEATURES] =
	{{0.299624935085,0.615929864316},{0.101052899654,0.611077400417},{-0.128646864804,-0.651213895138},{-0.650192143204,0.655302637611},{-0.993467350655,-0.610590903183},{-0.415232351895,0.324421587875},{-0.220861124636,-0.152991366478},{0.288675863647,-1.0},{0.281860078178,0.726376256235},{-0.109982188107,0.930917179895},{0.430032542189,0.276176897455},{-0.506788460657,0.695498652853},{0.93744434545,0.120857024333},{0.0463688323194,-0.692707692265},{0.709008960917,0.0311432851779},{0.497748411312,0.34411227062},{-0.923633440522,-0.627812447562},{-0.314531737989,0.716929036747},{-0.420153265105,-0.43745855748},{0.372106372622,0.124102768602},{-0.85015443191,-0.519022376389},{-0.413035718771,-0.465253503373},{-0.0259146397026,0.389514690615},{0.845648144604,0.0243919378152},{-0.136471450743,0.632002698107},{0.886301918722,-0.776697168051},{-0.581205639299,0.0502074920678},{-0.128580088863,0.244884056107},{0.98529866468,0.118332692033},{0.212218903897,0.922601702204},{-0.728260061232,0.331070600497},{0.98408087872,-0.109492545209},{0.85686053856,-0.557876382582},{-0.361058055931,-0.11621698087},{-0.655531081046,0.41333609297},{-0.102167119167,0.236472546457},{-0.708235286968,0.18842124283},{0.713854404333,-0.567720883285},{0.442127823682,0.76828090534},{0.0860988070203,-0.529040217669},{-0.383998998567,-0.52383365195},{-0.569308792803,-0.246680820013},{-0.871986436846,-0.127149563845},{0.0696207826784,-0.641545010171},{0.437066115437,-0.583443302135},{0.941055268814,0.644286094675},{-0.881757930032,0.307659576097},{-0.0892411392262,-0.424228898848},{-0.367938594118,-0.0471309774761},{0.80755161754,-0.759839850348},{-0.275288959166,0.00573017386269},{0.345008170036,-0.50274931809},{-0.869144322982,0.38930975149},{0.495443899125,-0.64501846646},{-1.0,-0.443656794124},{0.319782164809,0.664581575257},{-0.889444432103,1.0},{-0.749832248467,0.00759765657354},{0.439628663801,0.802145786686},{0.543480455062,-0.24692473483},{0.126739566219,0.966286559271},{-0.440214282121,-0.144358422706},{-0.398331101491,-0.554114793188},{0.586540409212,-0.225129525493},{0.531634772649,-0.4791850221},{0.835642656001,0.363950552554},{-0.0219682339886,0.473784028146}};
	unsigned y_train[] =
	{1,1,0,1,0,1,1,0,1,1,1,1,1,0,1,1,0,1,0,1,0,0,1,1,1,0,0,1,1,1,0,1,1,0,0,1,0,1,1,0,0,0,0,0,1,1,0,0,0,0,1,1,0,0,0,1,0,0,1,1,1,0,0,1,1,1,1};
	double x_test[][NB_FEATURES] =
	{{-0.0822793753912,0.938458198522},{0.196451675229,0.401874376517},{0.163917113215,0.825907982526},{0.763136088743,0.701956537975},{0.301978143474,0.752589124762},{0.733946624421,-0.625368228437},{0.273741846901,0.589128894218},{-0.927808154183,0.904172505703},{1.0,0.223621807557},{0.499517115232,-0.706589909541},{0.282243054908,-0.678606501846},{0.687140252822,0.148461213627},{0.731132678591,0.667166224207},{-0.295495975136,-0.366871706662},{0.392497449863,0.934901621353},{-0.249093959264,-0.746339688949},{0.35114989745,0.167633552706},{0.051695668102,-0.109716286215},{0.118299011024,-0.351444333771},{-0.0779905970306,-0.371351053508},{-0.369732368772,0.137594080924},{0.403886791846,0.310784289171},{-0.701909923654,0.96091035416},{0.288993688818,-0.532689479404},{-0.271214269586,-0.854468470825},{0.266747309854,0.149070075307},{0.0681080750347,0.0542876104273},{-0.880943214699,-0.601376058902},{-0.0766049419588,0.152875378084},{-0.454951880101,-0.376559493386},{-0.834064315375,0.239235575655},{-0.174931009836,0.32584509768},{0.558685661671,-0.621184853305}};
	unsigned y_test[] =
	{1,1,1,1,1,1,1,0,1,0,0,1,1,0,1,0,1,1,1,0,1,1,1,1,0,1,1,0,1,0,0,1,1};

	double alpha = 0.01;
	unsigned num_iters = 2000;
	double theta[NB_FEATURES] = {0,0};
	
	double trained_theta[NB_FEATURES] = {0};
	double *tmp = (double *) trained_theta;

	printf("before LR\n");
	// Train the classifier with dataset
	Logistic_regression(x_train, y_train, alpha, theta, num_iters, &tmp);
	printf("\n _TRAINED_theta values are : theta[0] = %f, theta[1] = %f \n", tmp[0], tmp[1]);

	double x_test_1d[TEST_SIZE*NB_FEATURES];
	for (i = 0; i < TEST_SIZE; i++){
		for (j = 0; j < NB_FEATURES; j++){
			x_test_1d[i*NB_FEATURES + j ] = x_test[i][j];
		}
	}

	unsigned length_test = sizeof(y_test)/sizeof(unsigned int);
	printf("length test %d\n",length_test);
	unsigned int x,jj; 
	unsigned prediction;
	unsigned score = 0; 
	double my_score;
	double *x_test_r = malloc (sizeof(double)* FEATURE_SIZE);
	double pred_sw, pred_hw;
	double output_hw[TEST_SIZE];
	static unsigned count;

	printf("Running DUT...");
	classify(x_test,tmp,output_hw);

	printf("done.\n");
	printf("Comparing results\n");
	for (x = 0; x < length_test; x++){
		for (jj = 0; jj < TEST_SIZE; jj++)
			x_test_r[jj] = x_test[x][jj];
		// Get Software results
		pred_sw = sw_classify(x_test_r,tmp);
		// Get Hardware results
		pred_hw = output_hw[x];
		// Compare both versions
		if (abs(pred_hw - pred_sw) >= 1e-6)
			printf("output differs for index = %d, SW = %a HW = %a\n", x,pred_sw, pred_hw), count++;
		else
			printf("HW: %lf, \t SW = %lf\n", pred_hw, pred_sw);
	}
	if (count == 0)
		printf("Test Passed Successfully\n");
	else
		printf("Test Failed\n");
	return 0;
}
